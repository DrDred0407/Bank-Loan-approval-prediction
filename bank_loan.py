# -*- coding: utf-8 -*-
"""Bank loan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H1HT-zkklNqcrPEvTjeWXobd7Po3qCaV

# **Predictiing Bank loan approval**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
import matplotlib
# %matplotlib inline

df = pd.read_csv("train.csv")
df.head()

df.info()

df.describe()

df.isnull().sum()

from sklearn.impute import SimpleImputer
num_var = df.select_dtypes(include = ['int64','float64']).columns
imp = SimpleImputer(strategy = 'mean')
df[num_var] = imp.fit_transform(df[num_var])
df.isnull().sum()

cat_var = df.select_dtypes(include=['O']).columns
imp_mode = SimpleImputer(strategy = 'most_frequent')
df[cat_var] = imp_mode.fit_transform(df[cat_var])
df.isnull().sum()

sns.countplot(df['Gender'])

sns.countplot(df['Married'])

sns.countplot(df['Dependents'])

sns.countplot(df['Education'])

sns.countplot(df['Property_Area'])

sns.countplot(df['Loan_Status'])

sns.distplot(df["ApplicantIncome"])

sns.distplot(df["CoapplicantIncome"])

sns.distplot(df["LoanAmount"])

df['Total_Income'] = df['ApplicantIncome'] + df['CoapplicantIncome']

# apply log transformation to the attribute to transform in bell curve as now model wont work properly
df['ApplicantIncomeLog'] = np.log(df['ApplicantIncome'])
sns.distplot(df["ApplicantIncomeLog"])

#apply log transformation when you see right or left skewed for model to work properly
df['CoapplicantIncomeLog'] = np.log(df['CoapplicantIncome']+1)
sns.distplot(df["CoapplicantIncomeLog"])

df['LoanAmountLog'] = np.log(df['LoanAmount'])
sns.distplot(df["LoanAmountLog"])

df['Loan_Amount_Term_Log'] = np.log(df['Loan_Amount_Term']+1)
sns.distplot(df["Loan_Amount_Term_Log"])

df['Total_Income_Log'] = np.log(df['Total_Income'])
sns.distplot(df["Total_Income_Log"])

corr = df.corr()
plt.figure(figsize=(15,10))
sns.heatmap(corr, annot = True, cmap="BuPu")

df.head()

cols = ['ApplicantIncome', 'CoapplicantIncome', "LoanAmount", "Loan_Amount_Term", "Total_Income", 'Loan_ID', 'CoapplicantIncomeLog']
df = df.drop(columns=cols, axis=1)
df.head()

from sklearn.preprocessing import LabelEncoder
cat = df.select_dtypes(include=['O']).columns

cat

label = LabelEncoder()
for col in cat:
    df[col] = label.fit_transform(df[col])

"""## **Train Test split**"""

X = df.drop(columns=['Loan_Status'], axis=1)
y = df['Loan_Status']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

"""## **Training**"""

from sklearn.model_selection import cross_val_score
def classify(model, x, y):
    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    model.fit(x_train, y_train)
    print("Accuracy is", model.score(x_test, y_test)*100)
    # cross validation - it is used for better validation of model
    # eg: cv-5, train-4, test-1
    score = cross_val_score(model, x, y, cv=5)
    print("Cross validation is",np.mean(score)*100)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
classify(model, X, y)

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
classify(model, X, y)

from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier
model = RandomForestClassifier()
classify(model, X, y)

model = ExtraTreesClassifier()
classify(model, X, y)

n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 10)]
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
min_samples_split = [2, 5, 10]
max_features = ['auto', 'sqrt']
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               }

from sklearn.model_selection import RandomizedSearchCV
rf = RandomForestClassifier()
rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)
rf_random.fit(X,y)

rf_random.best_params_

model = RandomForestClassifier(n_estimators=366, min_samples_split=5, max_depth=20, max_features='auto')
classify(model, X, y)

model.fit(x_train, y_train)

y_pred = model.predict(x_test)

y_pred

print("Accuracy: ",model.score(x_test, y_test) * 100)

from sklearn.metrics import confusion_matrix
y_pred = model.predict(x_test)
cm = confusion_matrix(y_test, y_pred)
cm

sns.heatmap(cm, annot=True)

